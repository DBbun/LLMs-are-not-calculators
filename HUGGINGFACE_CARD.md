---
license: mit
task_categories:
- educational-analysis
- behavioral-modeling
language:
- en
tags:
- education
- ai-impact
- llm
- student-behavior
- synthetic-data
size_categories:
- 1K<n<10K
---

# LLM Education Impact Dataset

## Dataset Description

**Developed by:** DBbun LLC  
**Based on:** Daniel Jackson (MIT, 2025) research on LLM usage in undergraduate software development courses

This dataset contains synthetic observational data simulating student interactions with different AI tools in educational settings. The data is grounded in research by Daniel Jackson (MIT, 2025) on LLM usage in undergraduate software development courses.

### Dataset Summary

The dataset captures:
- **120 students** with varying behavioral traits
- **15 educational tasks** (coding, writing, reading)
- **600 complete runs** of students completing tasks
- **~12,000 fine-grained events** tracking student activities
- **3 tool modes**: search engines, explicit-context LLMs, agentic LLMs

### Supported Tasks

- Educational outcome prediction
- Behavioral analysis
- Tool effectiveness comparison
- Agency and engagement measurement
- Error pattern identification

### Languages

English (en)

## Dataset Structure

### Data Instances

A typical run record:

```json
{
  "run_id": "R000042",
  "student_id": "S0015",
  "task_id": "T003",
  "tool_mode": "llm_explicit",
  "score": 0.78,
  "passed": 1,
  "reading_sessions": 2,
  "doc_opens": 5,
  "prompts": 4,
  "verification_checks": 3,
  "edits": 7,
  "hallucinations": 1,
  "hallucinations_caught": 1,
  "agency_proxy": 0.65,
  "cognitive_engagement": 0.71
}
```

### Data Fields

**students.csv**
- `student_id`: Unique identifier
- `skill`: Base ability (0-1)
- `effort_minimization`: Shortcut tendency (0-1)
- `doc_discipline`: Documentation reading propensity (0-1)
- `context_care`: Context selection attention (0-1)
- `time_pressure`: Stress level (0-1)
- `intentionality`: Deliberate tool use (0-1)

**tasks.csv**
- `task_id`: Unique identifier
- `task_type`: coding | writing | reading
- `difficulty`: 0-1 scale
- `has_module_boundaries`: Boolean (modularity requirement)
- `rubric_strictness`: 0-1 scale
- `requires_prior_reading`: Boolean
- `conceptual_complexity`: 0-1 scale

**runs.csv** (primary analysis table)
- Identifiers: `run_id`, `student_id`, `task_id`, `tool_mode`
- Behaviors: `prompts`, `doc_opens`, `reading_sessions`, `edits`, `test_runs`, `verification_checks`
- Errors: `hallucinations`, `omissions`, `boundary_violations` (+ caught variants)
- Outcomes: `score`, `passed`, `tests_failed`
- Learning proxies: `agency_proxy`, `cognitive_engagement`, `context_quality`

**events.csv**
- Fine-grained activity log with timestamps
- Event types: start, reading_session, doc_lookup, prompt, verify_output, edit, run_tests, submit
- Includes metadata for each event

### Data Splits

This is a complete generated dataset with no predefined splits. Users should create their own splits based on research needs.

Suggested splits:
- **By student**: 70/15/15 train/val/test
- **By task**: Leave-one-task-out cross-validation
- **By tool mode**: Within-tool and cross-tool evaluation

## Dataset Creation

### Curation Rationale

This dataset was created to support research on:
1. How different AI tools affect educational outcomes
2. Which student traits predict successful AI usage
3. How to preserve student agency with AI tools
4. The role of intentionality and verification in learning

### Source Data

#### Initial Data Collection and Normalization

This is synthetic data generated by a simulation based on:
- Jackson's MIT course experiments (Fall 2024)
- Behavioral observations from ~100 students
- Error rates from LLM benchmarks
- Educational psychology literature

#### Who are the source language producers?

The simulation models behaviors observed in undergraduate students at MIT taking software development courses.

### Annotations

#### Annotation process

All labels are generated by the simulation based on theoretical models:
- **Behavioral traits**: Sampled from distributions calibrated to real student populations
- **Outcomes (score, pass/fail)**: Calculated from weighted combination of behaviors and errors
- **Learning proxies**: Derived from observable activities using validated educational frameworks

#### Who are the annotators?

N/A - This is fully synthetic data

### Personal and Sensitive Information

This dataset contains **no personal information**. All student IDs are synthetic identifiers with no connection to real individuals.

## Considerations for Using the Data

### Social Impact of Dataset

This dataset aims to support research on educational AI that:
- Preserves student agency and learning
- Identifies effective vs harmful tool usage patterns
- Informs pedagogical strategies for AI integration
- Helps educators make evidence-based decisions

Potential risks:
- Over-generalization from synthetic data to real students
- Policy decisions without validation on real data
- Assuming all students fit the modeled distributions

### Discussion of Biases

**Modeled biases:**
- Parameters calibrated to MIT undergraduate population (may not generalize)
- Emphasis on software development tasks (Jackson's course context)
- Western educational assumptions

**Intentional design:**
- No demographic variables included (to avoid stereotyping)
- Trait distributions designed to be broad and diverse
- Multiple tool modes to avoid single-tool bias

### Other Known Limitations

1. **Synthetic data**: Patterns are theoretically motivated but not validated against large-scale real data
2. **Task scope**: Limited to coding, writing, and reading tasks
3. **Temporal**: No learning-over-time dynamics
4. **Social**: No peer interaction or collaborative work
5. **Assessment**: Single rubric-based scoring (real assessment is more complex)

Use this dataset as a **starting point** for hypothesis generation, not as definitive evidence for educational policy.

## Additional Information

### Dataset Curators

**Developed by:** DBbun LLC  
Based on research by Daniel Jackson (MIT CSAIL)  
Simulation implementation: Version 2.0

### Licensing Information

MIT License

### Citation Information

```bibtex
@article{jackson2025llms,
  title={LLMs are not calculators: Why educators should embrace AI (and fear it)},
  author={Jackson, Daniel},
  year={2025},
  month={December}
}

@dataset{llm_education_dataset,
  title={LLM Education Impact Dataset},
  author={DBbun LLC},
  note={Based on research by Jackson, Daniel},
  year={2026},
  publisher={Hugging Face},
  version={2.0}
}
```

### Contributions

Contributions welcome! See the [GitHub repository](https://github.com/yourusername/llm-education-simulator) for:
- Simulation code
- Configuration examples
- Analysis notebooks
- Issue tracker

### Contact

For questions or feedback:
- **Organization:** DBbun LLC
- Open an issue on GitHub
- Based on research by Daniel Jackson (MIT)

### Changelog

**Version 2.0** (2026-02-XX)
- Complete rewrite based on Jackson (2025) paper
- Added reading behavior tracking
- Added intentionality and context quality metrics
- Enhanced error tracking (caught vs uncaught)
- Improved theoretical grounding
- Added comprehensive documentation

**Version 1.0** (Previous)
- Initial release
- Basic behavioral simulation
